{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Scraping (legal way)\n",
    "\n",
    "Register app @ https://ssl.reddit.com/prefs/apps/ to scrape data through reddit's API\n",
    "\n",
    "Objective of scraping: Trends \n",
    "\n",
    "Key words: \n",
    "- Communities other than Keurig: brew, bean, Nespresso, nespresso, keurig, Keurig, espresso, machine, waste, sustainability, kcups, pod, reusable, traditional, drip, french press, single-serve, single serve, French press, nitro, cold brew, roast, specialty\n",
    "\n",
    "- Keurig community: Pods, cup, reusable \n",
    "\n",
    "\n",
    "Features of posts to be retained:\n",
    "- Title \n",
    "- Post \n",
    "- Date \n",
    "- URL \n",
    "- Score  \n",
    "- Comments (all comments) \n",
    "- Number of comments \n",
    "\n",
    "Note:\n",
    "1. Fill in the relevant client id, secret and user agent\n",
    "2. Replace the relevant subreddit name \n",
    "3. Rename the lists and file name as necessary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espresso extraction try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from saved progress: 194 priority posts, 0 non-priority posts.\n",
      "Starting scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/y17z3kl153g6yzxs84v9wh080000gn/T/ipykernel_39364/4087538398.py:73: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching to non-priority posts...\n",
      "Script interrupted. Saving progress...\n",
      "Progress saved. Exiting.\n",
      "Saved 252 priority posts.\n",
      "Saved 268 total posts.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Authenticate with Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"AxA88TAw9r4lzQrZqUk8hQ\",\n",
    "    client_secret=\"HUyuEV1pOkn2qnpmGrRxFqc6xMlR4w\",\n",
    "    user_agent=\"python:espresso (by /u/Fit-Blood1919)\"\n",
    ")\n",
    "\n",
    "# Access the subreddit\n",
    "subreddit = reddit.subreddit(\"espresso\")\n",
    "\n",
    "# List of keywords\n",
    "keywords = [\n",
    "    \"brew\", \"bean\", \"nespresso\", \"keurig\", \"espresso\", \"machine\",\n",
    "    \"waste\", \"sustainability\", \"kcups\", \"pod\", \"reusable\",\n",
    "    \"traditional\", \"drip\", \"french press\", \"single-serve\",\n",
    "    \"single serve\", \"k-cup\", \"roast\", \"specialty\", \"French press\",\n",
    "    \"nitro\", \"cold brew\"\n",
    "]\n",
    "\n",
    "# Files for saving progress\n",
    "priority_file = \"partial_priority_espresso_posts.csv\"\n",
    "non_priority_file = \"partial_non_priority_espresso_posts.csv\"\n",
    "\n",
    "# Load previous progress if available\n",
    "try:\n",
    "    priority_posts = pd.read_csv(priority_file).to_dict('records')\n",
    "    non_priority_posts = pd.read_csv(non_priority_file).to_dict('records')\n",
    "    post_ids = {post['id'] for post in priority_posts + non_priority_posts}\n",
    "    print(f\"Resuming from saved progress: {len(priority_posts)} priority posts, {len(non_priority_posts)} non-priority posts.\")\n",
    "except FileNotFoundError:\n",
    "    priority_posts, non_priority_posts, post_ids = [], [], set()\n",
    "    print(\"No saved progress found. Starting fresh.\")\n",
    "\n",
    "# Helper functions\n",
    "def contains_keywords(text):\n",
    "    return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "def save_to_csv(file_name, posts):\n",
    "    header = ['id', 'title', 'body', 'upvotes', 'url', 'created_time', 'num_comments', 'comments', 'contains_keywords']\n",
    "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(posts)\n",
    "\n",
    "def process_posts(posts, priority_only=False):\n",
    "    for post in posts:\n",
    "        if post.id not in post_ids:\n",
    "            try:\n",
    "                # Expand comments\n",
    "                post.comments.replace_more(limit=None)\n",
    "                comment_bodies = [comment.body for comment in post.comments.list()]\n",
    "                all_comments = \" | \".join(comment_bodies)\n",
    "\n",
    "                # Check for keywords\n",
    "                has_keywords = (\n",
    "                    contains_keywords(post.title)\n",
    "                    or contains_keywords(post.selftext)\n",
    "                    or any(contains_keywords(comment) for comment in comment_bodies)\n",
    "                )\n",
    "\n",
    "                post_data = {\n",
    "                    'id': post.id,\n",
    "                    'title': post.title,\n",
    "                    'body': post.selftext,\n",
    "                    'upvotes': post.score,\n",
    "                    'url': post.url,\n",
    "                    'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'comments': all_comments,\n",
    "                    'num_comments': len(comment_bodies),\n",
    "                    'contains_keywords': has_keywords\n",
    "                }\n",
    "\n",
    "                # Save to appropriate list\n",
    "                if has_keywords:\n",
    "                    priority_posts.append(post_data)\n",
    "                elif not priority_only:\n",
    "                    non_priority_posts.append(post_data)\n",
    "\n",
    "                post_ids.add(post.id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {post.id}: {e}\")\n",
    "\n",
    "# Parameters\n",
    "fetch_limit = 3000\n",
    "posts_per_request = 100\n",
    "pause_time = 2\n",
    "max_posts_to_screen = 2000\n",
    "checkpoint_interval = 500\n",
    "\n",
    "# Fetch posts\n",
    "try:\n",
    "    screened_posts_count = len(post_ids)  # Start from previously screened count\n",
    "    print(\"Starting scraping...\")\n",
    "\n",
    "    # Stage 1: Priority posts\n",
    "    while len(priority_posts) < fetch_limit and screened_posts_count < max_posts_to_screen:\n",
    "        posts = subreddit.hot(limit=posts_per_request)\n",
    "        process_posts(posts, priority_only=True)\n",
    "        screened_posts_count += posts_per_request\n",
    "\n",
    "        if screened_posts_count % checkpoint_interval == 0:\n",
    "            save_to_csv(priority_file, priority_posts)\n",
    "            print(f\"Checkpoint: {screened_posts_count} posts screened. Priority posts: {len(priority_posts)}.\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "    # Stage 2: Additional non-priority posts if needed\n",
    "    if len(priority_posts) < fetch_limit:\n",
    "        print(\"Switching to non-priority posts...\")\n",
    "        while len(priority_posts) + len(non_priority_posts) < fetch_limit:\n",
    "            posts = subreddit.hot(limit=posts_per_request)\n",
    "            process_posts(posts, priority_only=False)\n",
    "\n",
    "            if len(post_ids) % checkpoint_interval == 0:\n",
    "                save_to_csv(priority_file, priority_posts)\n",
    "                save_to_csv(non_priority_file, non_priority_posts)\n",
    "                print(f\"Checkpoint: Saved {len(priority_posts)} priority and {len(non_priority_posts)} non-priority posts.\")\n",
    "            time.sleep(pause_time)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Script interrupted. Saving progress...\")\n",
    "    save_to_csv(priority_file, priority_posts)\n",
    "    save_to_csv(non_priority_file, non_priority_posts)\n",
    "    print(\"Progress saved. Exiting.\")\n",
    "\n",
    "# Final Save\n",
    "save_to_csv(\"espresso_priority_posts_3000.csv\", priority_posts)\n",
    "print(f\"Saved {len(priority_posts)} priority posts.\")\n",
    "\n",
    "save_to_csv(\"espresso_all_posts_3000.csv\", priority_posts + non_priority_posts)\n",
    "print(f\"Saved {len(priority_posts) + len(non_priority_posts)} total posts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nespresso subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved progress found. Starting fresh.\n",
      "Starting scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/y17z3kl153g6yzxs84v9wh080000gn/T/ipykernel_39364/359801943.py:73: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 1000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 1500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 2000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 2500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 3000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 3500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 4000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 4500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 5000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 5500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 6000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 6500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 7000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 7500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 8000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 8500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 9000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 9500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 10000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 10500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 11000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 11500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 12000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 12500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 13000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 13500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 14000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 14500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 15000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 15500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 16000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 16500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 17000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 17500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 18000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 18500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 19000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 19500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 20000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 20500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 21000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 21500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 22000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 22500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 23000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 23500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 24000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 24500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 25000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 25500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 26000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 26500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 27000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 27500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 28000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 28500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 29000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 29500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 30000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 30500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 31000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 31500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 32000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 32500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 33000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 33500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 34000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 34500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 35000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 35500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 36000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 36500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 37000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 37500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 38000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 38500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 39000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 39500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 40000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 40500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 41000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 41500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 42000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 42500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 43000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 43500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 44000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 44500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 45000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 45500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 46000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 46500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 47000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 47500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 48000 posts screened. Priority posts: 90.\n",
      "Checkpoint: 48500 posts screened. Priority posts: 90.\n",
      "Checkpoint: 49000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 49500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 50000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 50500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 51000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 51500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 52000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 52500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 53000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 53500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 54000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 54500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 55000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 55500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 56000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 56500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 57000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 57500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 58000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 58500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 59000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 59500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 60000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 60500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 61000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 61500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 62000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 62500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 63000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 63500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 64000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 64500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 65000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 65500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 66000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 66500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 67000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 67500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 68000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 68500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 69000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 69500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 70000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 70500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 71000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 71500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 72000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 72500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 73000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 73500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 74000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 74500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 75000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 75500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 76000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 76500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 77000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 77500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 78000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 78500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 79000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 79500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 80000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 80500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 81000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 81500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 82000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 82500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 83000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 83500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 84000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 84500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 85000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 85500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 86000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 86500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 87000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 87500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 88000 posts screened. Priority posts: 91.\n",
      "Checkpoint: 88500 posts screened. Priority posts: 91.\n",
      "Checkpoint: 89000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 89500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 90000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 90500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 91000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 91500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 92000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 92500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 93000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 93500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 94000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 94500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 95000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 95500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 96000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 96500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 97000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 97500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 98000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 98500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 99000 posts screened. Priority posts: 92.\n",
      "Checkpoint: 99500 posts screened. Priority posts: 92.\n",
      "Checkpoint: 100000 posts screened. Priority posts: 92.\n",
      "Switching to non-priority posts...\n",
      "Script interrupted. Saving progress...\n",
      "Progress saved. Exiting.\n",
      "Saved 106 priority posts.\n",
      "Saved 109 total posts.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Authenticate with Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"-tX-5xECMB1d2cgNSs2ysg\",\n",
    "    client_secret=\"sQlh4Z2M9BBkCwvGFAS08N_6_0Xr2g\",\n",
    "    user_agent=\"python:nespresso (by /u/Fit-Blood1919)\"\n",
    ")\n",
    "\n",
    "# Access the subreddit\n",
    "subreddit = reddit.subreddit(\"nespresso\")\n",
    "\n",
    "# List of keywords\n",
    "keywords = [\n",
    "    \"brew\", \"bean\", \"nespresso\", \"keurig\", \"espresso\", \"machine\",\n",
    "    \"waste\", \"sustainability\", \"kcups\", \"pod\", \"reusable\",\n",
    "    \"traditional\", \"drip\", \"french press\", \"single-serve\",\n",
    "    \"single serve\", \"k-cup\", \"roast\", \"specialty\", \"French press\",\n",
    "    \"nitro\", \"cold brew\"\n",
    "]\n",
    "\n",
    "# Files for saving progress\n",
    "priority_file = \"partial_priority_nespresso_posts.csv\"\n",
    "non_priority_file = \"partial_non_priority_nespresso_posts.csv\"\n",
    "\n",
    "# Load previous progress if available\n",
    "try:\n",
    "    priority_posts = pd.read_csv(priority_file).to_dict('records')\n",
    "    non_priority_posts = pd.read_csv(non_priority_file).to_dict('records')\n",
    "    post_ids = {post['id'] for post in priority_posts + non_priority_posts}\n",
    "    print(f\"Resuming from saved progress: {len(priority_posts)} priority posts, {len(non_priority_posts)} non-priority posts.\")\n",
    "except FileNotFoundError:\n",
    "    priority_posts, non_priority_posts, post_ids = [], [], set()\n",
    "    print(\"No saved progress found. Starting fresh.\")\n",
    "\n",
    "# Helper functions\n",
    "def contains_keywords(text):\n",
    "    return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "def save_to_csv(file_name, posts):\n",
    "    header = ['id', 'title', 'body', 'upvotes', 'url', 'created_time', 'num_comments', 'comments', 'contains_keywords']\n",
    "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(posts)\n",
    "\n",
    "def process_posts(posts, priority_only=False):\n",
    "    for post in posts:\n",
    "        if post.id not in post_ids:\n",
    "            try:\n",
    "                # Expand comments\n",
    "                post.comments.replace_more(limit=None)\n",
    "                comment_bodies = [comment.body for comment in post.comments.list()]\n",
    "                all_comments = \" | \".join(comment_bodies)\n",
    "\n",
    "                # Check for keywords\n",
    "                has_keywords = (\n",
    "                    contains_keywords(post.title)\n",
    "                    or contains_keywords(post.selftext)\n",
    "                    or any(contains_keywords(comment) for comment in comment_bodies)\n",
    "                )\n",
    "\n",
    "                post_data = {\n",
    "                    'id': post.id,\n",
    "                    'title': post.title,\n",
    "                    'body': post.selftext,\n",
    "                    'upvotes': post.score,\n",
    "                    'url': post.url,\n",
    "                    'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'comments': all_comments,\n",
    "                    'num_comments': len(comment_bodies),\n",
    "                    'contains_keywords': has_keywords\n",
    "                }\n",
    "\n",
    "                # Save to appropriate list\n",
    "                if has_keywords:\n",
    "                    priority_posts.append(post_data)\n",
    "                elif not priority_only:\n",
    "                    non_priority_posts.append(post_data)\n",
    "\n",
    "                post_ids.add(post.id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {post.id}: {e}\")\n",
    "\n",
    "# Parameters\n",
    "fetch_limit = 2000\n",
    "posts_per_request = 100\n",
    "pause_time = 2\n",
    "max_posts_to_screen = 100000\n",
    "checkpoint_interval = 500\n",
    "\n",
    "# Fetch posts\n",
    "try:\n",
    "    screened_posts_count = len(post_ids)  # Start from previously screened count\n",
    "    print(\"Starting scraping...\")\n",
    "\n",
    "    # Stage 1: Priority posts\n",
    "    while len(priority_posts) < 500 and screened_posts_count < max_posts_to_screen:\n",
    "        posts = subreddit.hot(limit=posts_per_request)\n",
    "        process_posts(posts, priority_only=True)\n",
    "        screened_posts_count += posts_per_request\n",
    "\n",
    "        if screened_posts_count % checkpoint_interval == 0:\n",
    "            save_to_csv(priority_file, priority_posts)\n",
    "            print(f\"Checkpoint: {screened_posts_count} posts screened. Priority posts: {len(priority_posts)}.\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "    # Stage 2: Additional non-priority posts if needed\n",
    "    if len(priority_posts) < fetch_limit:\n",
    "        print(\"Switching to non-priority posts...\")\n",
    "        while len(priority_posts) + len(non_priority_posts) < fetch_limit:\n",
    "            posts = subreddit.hot(limit=posts_per_request)\n",
    "            process_posts(posts, priority_only=False)\n",
    "\n",
    "            if len(post_ids) % checkpoint_interval == 0:\n",
    "                save_to_csv(priority_file, priority_posts)\n",
    "                save_to_csv(non_priority_file, non_priority_posts)\n",
    "                print(f\"Checkpoint: Saved {len(priority_posts)} priority and {len(non_priority_posts)} non-priority posts.\")\n",
    "            time.sleep(pause_time)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Script interrupted. Saving progress...\")\n",
    "    save_to_csv(priority_file, priority_posts)\n",
    "    save_to_csv(non_priority_file, non_priority_posts)\n",
    "    print(\"Progress saved. Exiting.\")\n",
    "\n",
    "# Final Save\n",
    "save_to_csv(\"nespresso_priority_posts_3000.csv\", priority_posts)\n",
    "print(f\"Saved {len(priority_posts)} priority posts.\")\n",
    "\n",
    "save_to_csv(\"nespresso_all_posts_3000.csv\", priority_posts + non_priority_posts)\n",
    "print(f\"Saved {len(priority_posts) + len(non_priority_posts)} total posts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# James Hoffman (content creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from saved progress: 86 priority posts, 0 non-priority posts.\n",
      "Starting scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/y17z3kl153g6yzxs84v9wh080000gn/T/ipykernel_39364/2062233313.py:73: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching to non-priority posts...\n",
      "Script interrupted. Saving progress...\n",
      "Progress saved. Exiting.\n",
      "Saved 89 priority posts.\n",
      "Saved 89 total posts.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Authenticate with Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xXTkGHZ2iN_dO47QiHTA7w\",\n",
    "    client_secret=\"3wVSZ7sNP5wezm0kKxW7Wtu67DnA6w\",\n",
    "    user_agent=\"python:JamesHoffman (by /u/Fit-Blood1919)\"\n",
    ")\n",
    "\n",
    "# Access the subreddit\n",
    "subreddit = reddit.subreddit(\"JamesHoffmann\")\n",
    "\n",
    "# List of keywords\n",
    "keywords = [\n",
    "    \"brew\", \"bean\", \"nespresso\", \"keurig\", \"espresso\", \"machine\",\n",
    "    \"waste\", \"sustainability\", \"kcups\", \"pod\", \"reusable\",\n",
    "    \"traditional\", \"drip\", \"french press\", \"single-serve\",\n",
    "    \"single serve\", \"k-cup\", \"roast\", \"specialty\", \"French press\",\n",
    "    \"nitro\", \"cold brew\"\n",
    "]\n",
    "\n",
    "# Files for saving progress\n",
    "priority_file = \"partial_priority_jameshoff_posts.csv\"\n",
    "non_priority_file = \"partial_non_priority_jameshoff_posts.csv\"\n",
    "\n",
    "# Load previous progress if available\n",
    "try:\n",
    "    priority_posts = pd.read_csv(priority_file).to_dict('records')\n",
    "    non_priority_posts = pd.read_csv(non_priority_file).to_dict('records')\n",
    "    post_ids = {post['id'] for post in priority_posts + non_priority_posts}\n",
    "    print(f\"Resuming from saved progress: {len(priority_posts)} priority posts, {len(non_priority_posts)} non-priority posts.\")\n",
    "except FileNotFoundError:\n",
    "    priority_posts, non_priority_posts, post_ids = [], [], set()\n",
    "    print(\"No saved progress found. Starting fresh.\")\n",
    "\n",
    "# Helper functions\n",
    "def contains_keywords(text):\n",
    "    return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "def save_to_csv(file_name, posts):\n",
    "    header = ['id', 'title', 'body', 'upvotes', 'url', 'created_time', 'num_comments', 'comments', 'contains_keywords']\n",
    "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(posts)\n",
    "\n",
    "def process_posts(posts, priority_only=False):\n",
    "    for post in posts:\n",
    "        if post.id not in post_ids:\n",
    "            try:\n",
    "                # Expand comments\n",
    "                post.comments.replace_more(limit=None)\n",
    "                comment_bodies = [comment.body for comment in post.comments.list()]\n",
    "                all_comments = \" | \".join(comment_bodies)\n",
    "\n",
    "                # Check for keywords\n",
    "                has_keywords = (\n",
    "                    contains_keywords(post.title)\n",
    "                    or contains_keywords(post.selftext)\n",
    "                    or any(contains_keywords(comment) for comment in comment_bodies)\n",
    "                )\n",
    "\n",
    "                post_data = {\n",
    "                    'id': post.id,\n",
    "                    'title': post.title,\n",
    "                    'body': post.selftext,\n",
    "                    'upvotes': post.score,\n",
    "                    'url': post.url,\n",
    "                    'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'comments': all_comments,\n",
    "                    'num_comments': len(comment_bodies),\n",
    "                    'contains_keywords': has_keywords\n",
    "                }\n",
    "\n",
    "                # Save to appropriate list\n",
    "                if has_keywords:\n",
    "                    priority_posts.append(post_data)\n",
    "                elif not priority_only:\n",
    "                    non_priority_posts.append(post_data)\n",
    "\n",
    "                post_ids.add(post.id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {post.id}: {e}\")\n",
    "\n",
    "# Parameters\n",
    "fetch_limit = 3000\n",
    "posts_per_request = 100\n",
    "pause_time = 2\n",
    "max_posts_to_screen = 100000\n",
    "checkpoint_interval = 500\n",
    "\n",
    "# Fetch posts\n",
    "try:\n",
    "    screened_posts_count = len(post_ids)  # Start from previously screened count\n",
    "    print(\"Starting scraping...\")\n",
    "\n",
    "    # Stage 1: Priority posts\n",
    "    while len(priority_posts) < fetch_limit and screened_posts_count < max_posts_to_screen:\n",
    "        posts = subreddit.hot(limit=posts_per_request)\n",
    "        process_posts(posts, priority_only=True)\n",
    "        screened_posts_count += posts_per_request\n",
    "\n",
    "        if screened_posts_count % checkpoint_interval == 0:\n",
    "            save_to_csv(priority_file, priority_posts)\n",
    "            print(f\"Checkpoint: {screened_posts_count} posts screened. Priority posts: {len(priority_posts)}.\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "    # Stage 2: Additional non-priority posts if needed\n",
    "    if len(priority_posts) < fetch_limit:\n",
    "        print(\"Switching to non-priority posts...\")\n",
    "        while len(priority_posts) + len(non_priority_posts) < fetch_limit:\n",
    "            posts = subreddit.hot(limit=posts_per_request)\n",
    "            process_posts(posts, priority_only=False)\n",
    "\n",
    "            if len(post_ids) % checkpoint_interval == 0:\n",
    "                save_to_csv(priority_file, priority_posts)\n",
    "                save_to_csv(non_priority_file, non_priority_posts)\n",
    "                print(f\"Checkpoint: Saved {len(priority_posts)} priority and {len(non_priority_posts)} non-priority posts.\")\n",
    "            time.sleep(pause_time)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Script interrupted. Saving progress...\")\n",
    "    save_to_csv(priority_file, priority_posts)\n",
    "    save_to_csv(non_priority_file, non_priority_posts)\n",
    "    print(\"Progress saved. Exiting.\")\n",
    "\n",
    "# Final Save\n",
    "save_to_csv(\"jameshoff_priority_posts_3000.csv\", priority_posts)\n",
    "print(f\"Saved {len(priority_posts)} priority posts.\")\n",
    "\n",
    "save_to_csv(\"jameshoff_all_posts_3000.csv\", priority_posts + non_priority_posts)\n",
    "print(f\"Saved {len(priority_posts) + len(non_priority_posts)} total posts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pourover webscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved progress found. Starting fresh.\n",
      "Starting scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/y17z3kl153g6yzxs84v9wh080000gn/T/ipykernel_39364/2134783102.py:73: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 500 posts screened. Priority posts: 96.\n",
      "Checkpoint: 1000 posts screened. Priority posts: 96.\n",
      "Checkpoint: 1500 posts screened. Priority posts: 96.\n",
      "Checkpoint: 2000 posts screened. Priority posts: 99.\n",
      "Checkpoint: 2500 posts screened. Priority posts: 99.\n",
      "Checkpoint: 3000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 3500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 4000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 4500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 5000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 5500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 6000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 6500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 7000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 7500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 8000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 8500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 9000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 9500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 10000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 10500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 11000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 11500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 12000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 12500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 13000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 13500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 14000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 14500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 15000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 15500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 16000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 16500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 17000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 17500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 18000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 18500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 19000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 19500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 20000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 20500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 21000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 21500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 22000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 22500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 23000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 23500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 24000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 24500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 25000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 25500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 26000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 26500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 27000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 27500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 28000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 28500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 29000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 29500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 30000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 30500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 31000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 31500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 32000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 32500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 33000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 33500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 34000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 34500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 35000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 35500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 36000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 36500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 37000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 37500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 38000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 38500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 39000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 39500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 40000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 40500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 41000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 41500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 42000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 42500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 43000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 43500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 44000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 44500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 45000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 45500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 46000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 46500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 47000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 47500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 48000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 48500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 49000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 49500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 50000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 50500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 51000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 51500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 52000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 52500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 53000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 53500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 54000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 54500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 55000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 55500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 56000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 56500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 57000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 57500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 58000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 58500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 59000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 59500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 60000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 60500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 61000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 61500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 62000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 62500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 63000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 63500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 64000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 64500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 65000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 65500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 66000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 66500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 67000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 67500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 68000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 68500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 69000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 69500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 70000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 70500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 71000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 71500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 72000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 72500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 73000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 73500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 74000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 74500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 75000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 75500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 76000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 76500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 77000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 77500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 78000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 78500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 79000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 79500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 80000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 80500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 81000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 81500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 82000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 82500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 83000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 83500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 84000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 84500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 85000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 85500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 86000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 86500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 87000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 87500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 88000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 88500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 89000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 89500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 90000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 90500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 91000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 91500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 92000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 92500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 93000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 93500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 94000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 94500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 95000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 95500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 96000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 96500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 97000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 97500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 98000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 98500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 99000 posts screened. Priority posts: 100.\n",
      "Checkpoint: 99500 posts screened. Priority posts: 100.\n",
      "Checkpoint: 100000 posts screened. Priority posts: 100.\n",
      "Switching to non-priority posts...\n",
      "Script interrupted. Saving progress...\n",
      "Progress saved. Exiting.\n",
      "Saved 102 priority posts.\n",
      "Saved 102 total posts.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Authenticate with Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"59_GDgLd2eBJ7RQTe4HgFQ\",\n",
    "    client_secret=\"Wsp5z2g7gos8M38vTU74uWA_zn4smg\",\n",
    "    user_agent=\"python:pourover (by /u/granola-cookies)\"\n",
    ")\n",
    "\n",
    "# Access the subreddit\n",
    "subreddit = reddit.subreddit(\"pourover\")\n",
    "\n",
    "# List of keywords\n",
    "keywords = [\n",
    "    \"brew\", \"bean\", \"nespresso\", \"keurig\", \"espresso\", \"machine\",\n",
    "    \"waste\", \"sustainability\", \"kcups\", \"pod\", \"reusable\",\n",
    "    \"traditional\", \"drip\", \"french press\", \"single-serve\",\n",
    "    \"single serve\", \"k-cup\", \"roast\", \"specialty\", \"French press\",\n",
    "    \"nitro\", \"cold brew\"\n",
    "]\n",
    "\n",
    "# Files for saving progress\n",
    "priority_file = \"partial_priority_pourover_posts.csv\"\n",
    "non_priority_file = \"partial_non_priority_pourover_posts.csv\"\n",
    "\n",
    "# Load previous progress if available\n",
    "try:\n",
    "    priority_posts = pd.read_csv(priority_file).to_dict('records')\n",
    "    non_priority_posts = pd.read_csv(non_priority_file).to_dict('records')\n",
    "    post_ids = {post['id'] for post in priority_posts + non_priority_posts}\n",
    "    print(f\"Resuming from saved progress: {len(priority_posts)} priority posts, {len(non_priority_posts)} non-priority posts.\")\n",
    "except FileNotFoundError:\n",
    "    priority_posts, non_priority_posts, post_ids = [], [], set()\n",
    "    print(\"No saved progress found. Starting fresh.\")\n",
    "\n",
    "# Helper functions\n",
    "def contains_keywords(text):\n",
    "    return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "def save_to_csv(file_name, posts):\n",
    "    header = ['id', 'title', 'body', 'upvotes', 'url', 'created_time', 'num_comments', 'comments', 'contains_keywords']\n",
    "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(posts)\n",
    "\n",
    "def process_posts(posts, priority_only=False):\n",
    "    for post in posts:\n",
    "        if post.id not in post_ids:\n",
    "            try:\n",
    "                # Expand comments\n",
    "                post.comments.replace_more(limit=None)\n",
    "                comment_bodies = [comment.body for comment in post.comments.list()]\n",
    "                all_comments = \" | \".join(comment_bodies)\n",
    "\n",
    "                # Check for keywords\n",
    "                has_keywords = (\n",
    "                    contains_keywords(post.title)\n",
    "                    or contains_keywords(post.selftext)\n",
    "                    or any(contains_keywords(comment) for comment in comment_bodies)\n",
    "                )\n",
    "\n",
    "                post_data = {\n",
    "                    'id': post.id,\n",
    "                    'title': post.title,\n",
    "                    'body': post.selftext,\n",
    "                    'upvotes': post.score,\n",
    "                    'url': post.url,\n",
    "                    'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'comments': all_comments,\n",
    "                    'num_comments': len(comment_bodies),\n",
    "                    'contains_keywords': has_keywords\n",
    "                }\n",
    "\n",
    "                # Save to appropriate list\n",
    "                if has_keywords:\n",
    "                    priority_posts.append(post_data)\n",
    "                elif not priority_only:\n",
    "                    non_priority_posts.append(post_data)\n",
    "\n",
    "                post_ids.add(post.id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {post.id}: {e}\")\n",
    "\n",
    "# Parameters\n",
    "fetch_limit = 3000\n",
    "posts_per_request = 100\n",
    "pause_time = 2\n",
    "max_posts_to_screen = 100000\n",
    "checkpoint_interval = 500\n",
    "\n",
    "# Fetch posts\n",
    "try:\n",
    "    screened_posts_count = len(post_ids)  # Start from previously screened count\n",
    "    print(\"Starting scraping...\")\n",
    "\n",
    "    # Stage 1: Priority posts\n",
    "    while len(priority_posts) < fetch_limit and screened_posts_count < max_posts_to_screen:\n",
    "        posts = subreddit.hot(limit=posts_per_request)\n",
    "        process_posts(posts, priority_only=True)\n",
    "        screened_posts_count += posts_per_request\n",
    "\n",
    "        if screened_posts_count % checkpoint_interval == 0:\n",
    "            save_to_csv(priority_file, priority_posts)\n",
    "            print(f\"Checkpoint: {screened_posts_count} posts screened. Priority posts: {len(priority_posts)}.\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "    # Stage 2: Additional non-priority posts if needed\n",
    "    if len(priority_posts) < fetch_limit:\n",
    "        print(\"Switching to non-priority posts...\")\n",
    "        while len(priority_posts) + len(non_priority_posts) < fetch_limit:\n",
    "            posts = subreddit.hot(limit=posts_per_request)\n",
    "            process_posts(posts, priority_only=False)\n",
    "\n",
    "            if len(post_ids) % checkpoint_interval == 0:\n",
    "                save_to_csv(priority_file, priority_posts)\n",
    "                save_to_csv(non_priority_file, non_priority_posts)\n",
    "                print(f\"Checkpoint: Saved {len(priority_posts)} priority and {len(non_priority_posts)} non-priority posts.\")\n",
    "            time.sleep(pause_time)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Script interrupted. Saving progress...\")\n",
    "    save_to_csv(priority_file, priority_posts)\n",
    "    save_to_csv(non_priority_file, non_priority_posts)\n",
    "    print(\"Progress saved. Exiting.\")\n",
    "\n",
    "# Final Save\n",
    "save_to_csv(\"pourover_priority_posts_3000.csv\", priority_posts)\n",
    "print(f\"Saved {len(priority_posts)} priority posts.\")\n",
    "\n",
    "save_to_csv(\"pourover_all_posts_3000.csv\", priority_posts + non_priority_posts)\n",
    "print(f\"Saved {len(priority_posts) + len(non_priority_posts)} total posts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushshift access is now restricted to moderators. No longer an option for webscrape. However, if we want old data from pushshift, code template from\n",
    "https://github.com/Watchful1/PushshiftDumps/tree/master "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test code with separate subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/y17z3kl153g6yzxs84v9wh080000gn/T/ipykernel_72360/811905476.py:58: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 214 posts and saved them to coffee_posts.csv.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# List of 2 different API credentials\n",
    "api_credentials = [\n",
    "    {\n",
    "        \"client_id\": \"RwYC7skcILM5paQFQd50fQ\",\n",
    "        \"client_secret\": \"T_Qo40-37JL3cPu5MunSsMWICOf8DQ\",\n",
    "        \"user_agent\": \"python:climbingv1.0 (by /u/granola-cookies)\"\n",
    "    },\n",
    "    {\n",
    "        \"client_id\": \"DPNTcZUVX6yOxJ30K5L_RQ\",\n",
    "        \"client_secret\": \"rw4ah7szqMrd7wpnI8zk8Q_wkiXYTQ\",\n",
    "        \"user_agent\": \"python:climbingv2.0 (by /u/granola-cookies)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize subreddit\n",
    "subreddit = \"Coffee\"\n",
    "keywords = [\"brew\", \"bean\", \"Keurig\", \"espresso\", \"machine\", \"waste\", \"sustainability\", \"kcups\", \"pod\", \n",
    "            \"reusable\", \"traditional\", \"drip\", \"French press\", \"single-serve\"]\n",
    "\n",
    "# List to store posts\n",
    "coffee_posts = []\n",
    "post_ids = set()  # To track unique post IDs and avoid duplicates\n",
    "\n",
    "# Variable to track pagination (after parameter)\n",
    "after = None\n",
    "\n",
    "# Function to check if a post/comment contains any of the keywords\n",
    "def contains_keywords(text):\n",
    "    return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "# Function to process and add posts\n",
    "def process_posts(posts, include_non_keyword=False):\n",
    "    global coffee_posts, post_ids  # Make sure these lists are updated globally\n",
    "    for post in posts:\n",
    "        if post.id not in post_ids:  # Check for duplicates\n",
    "            try:\n",
    "                # Check if the post title or body contains any of the keywords\n",
    "                match = contains_keywords(post.title) or contains_keywords(post.selftext)\n",
    "\n",
    "                # If it's a keyword match, include it, otherwise check if we're including non-matching posts\n",
    "                if match or (include_non_keyword and not match):\n",
    "                    post.comments.replace_more(limit=None)  # Expand comments\n",
    "                    comment_bodies = [comment.body for comment in post.comments.list()]\n",
    "                    all_comments = \" | \".join(comment_bodies)\n",
    "\n",
    "                    # Check if any comment contains the keywords (optional)\n",
    "                    if any(contains_keywords(comment) for comment in comment_bodies):\n",
    "                        coffee_posts.append({\n",
    "                            'title': post.title,\n",
    "                            'body': post.selftext,\n",
    "                            'upvotes': post.score,\n",
    "                            'url': post.url,\n",
    "                            'created_time': datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            'comments': all_comments,\n",
    "                            'num_comments': len(comment_bodies)\n",
    "                        })\n",
    "                        post_ids.add(post.id)  # Mark the post as processed\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {post.id}: {e}\")\n",
    "\n",
    "# Function to switch between API credentials\n",
    "def get_reddit_instance(credentials):\n",
    "    return praw.Reddit(\n",
    "        client_id=credentials[\"client_id\"],\n",
    "        client_secret=credentials[\"client_secret\"],\n",
    "        user_agent=credentials[\"user_agent\"]\n",
    "    )\n",
    "\n",
    "# Fetch posts using different API credentials and pagination\n",
    "def fetch_posts():\n",
    "    global after, current_api_index  # Ensure these are updated globally\n",
    "    target_post_count = 2000  # Set the target post count to 2000\n",
    "\n",
    "    while len(coffee_posts) < target_post_count:\n",
    "        try:\n",
    "            reddit = get_reddit_instance(api_credentials[current_api_index])\n",
    "            subreddit_instance = reddit.subreddit(subreddit)\n",
    "\n",
    "            # Search for posts using a broad query (will filter by keywords later)\n",
    "            subreddit_posts = list(subreddit_instance.search(' '.join(keywords), limit=100, params={\"after\": after}))\n",
    "\n",
    "            # Process the fetched posts (filter by keywords first)\n",
    "            process_posts(subreddit_posts, include_non_keyword=True)  # Process posts without keywords if necessary\n",
    "\n",
    "            # Update 'after' for pagination\n",
    "            if subreddit_posts:\n",
    "                after = subreddit_posts[-1].name\n",
    "            else:\n",
    "                break  # Stop if no more posts\n",
    "\n",
    "            # Rotate API credentials (to prevent hitting rate limits)\n",
    "            current_api_index = (current_api_index + 1) % len(api_credentials)\n",
    "\n",
    "            time.sleep(1)  # Add delay to prevent hitting the rate limit\n",
    "        except praw.exceptions.APIException as e:\n",
    "            if \"TooManyRequests\" in str(e):\n",
    "                print(\"Rate limit exceeded. Waiting for 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            break\n",
    "\n",
    "# Initialize current_api_index globally\n",
    "current_api_index = 0  # Set initial API index to 0 (use first API credentials)\n",
    "\n",
    "# Fetch posts with the main function\n",
    "fetch_posts()\n",
    "\n",
    "# Save the posts data to a CSV file\n",
    "csv_file = 'coffee_posts.csv'\n",
    "header = ['title', 'body', 'upvotes', 'url', 'created_time', 'num_comments', 'comments']\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header)\n",
    "    writer.writeheader()  # Write header\n",
    "    for post in coffee_posts:\n",
    "        writer.writerow(post)  # Write each post's data\n",
    "\n",
    "print(f\"Fetched {len(coffee_posts)} posts and saved them to {csv_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
